{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alice', 'adventures', 'in', 'wonderland', 'by', 'lewis', 'carroll', 'the', 'millennium', 'fulcrum', 'edition', '3', 'contents', 'chapter', 'i', 'down', 'the', 'rabbit', 'chapter', 'ii', 'the', 'pool', 'of', 'tears', 'chapter']\n",
      "corpus len:  25320\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "f = open('alice_in_wonderland.txt','r')\n",
    "while(1):\n",
    "    line =  f.readline()\n",
    "    if len(line) == 0: break\n",
    "    corpus.extend(line.split())\n",
    "        \n",
    "f.close()\n",
    "corpus = ' '.join(corpus)\n",
    "\n",
    "def clean_word(word):\n",
    "    word = word.lower()\n",
    "    for punctuation in ['\"',\"'\",'.',',','-','?','!',';',':','â€”','(',')','[',']']:\n",
    "        word = word.split(punctuation)[0]\n",
    "    return word\n",
    "\n",
    "\n",
    "\n",
    "corpus = [clean_word(word) for word in corpus.split()]\n",
    "corpus = [word for word in corpus if len(word) > 0]\n",
    "print(corpus[:25])\n",
    "D = len(corpus)\n",
    "print('corpus len: ',D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word list size (number of distinct words):  2637\n"
     ]
    }
   ],
   "source": [
    "tokenize = {}\n",
    "wordlist = []\n",
    "token = 0\n",
    "for word in corpus:\n",
    "    if word not in tokenize.keys():\n",
    "        tokenize[word] = token\n",
    "        wordlist.append(word)\n",
    "        token += 1\n",
    "    \n",
    "V = len(wordlist)\n",
    "\n",
    "# print(wordlist)\n",
    "# print(tokenize)\n",
    "# print(token)\n",
    "\n",
    "print('word list size (number of distinct words): ', V)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [9. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# bin how many times a word follows another word\n",
    "\n",
    "#rows are current word\n",
    "#columns are previous word\n",
    "counts_2gram = np.zeros((V,V))\n",
    "for i in range(1,len(corpus)):\n",
    "    token_i = tokenize[corpus[i]]\n",
    "    token_im1 = tokenize[corpus[i-1]]\n",
    "    counts_2gram[token_i,token_im1] += 1\n",
    "print(counts_2gram)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('was', 0.0007109004739336493)\n",
      "('queen', 0.002764612954186414)\n",
      "('cat', 0.00019747235387045816)\n",
      "('turtle', 0.0022511848341232226)\n",
      "('and', 0.00015797788309636652)\n",
      "('said', 0.0001579778830963665)\n",
      "Classification Accuracy of the Bigram Classifier:  0.2500098740076622\n"
     ]
    }
   ],
   "source": [
    "#past word as feature\n",
    "\n",
    "posterior_1word = np.zeros((V, V))\n",
    "prior = np.zeros(V)\n",
    "\n",
    "def get_likelihood_2gram(word):\n",
    "    # transpose counts_2gram to get rows as previous word\n",
    "    counts_2gram_t = counts_2gram.T\n",
    "    wordIndex = tokenize[word]\n",
    "    row = counts_2gram_t[wordIndex]\n",
    "    posterior_1word[wordIndex] = row / np.sum(row)\n",
    "    prior[wordIndex] = np.sum(row) / D\n",
    "    return posterior_1word[wordIndex] * prior[wordIndex]\n",
    "\n",
    "def pred_2gram(word):\n",
    "    likelihood = get_likelihood_2gram(word)\n",
    "    i = np.argmax(likelihood)\n",
    "    return(wordlist[i], likelihood[i])\n",
    "    \n",
    "print(pred_2gram('alice'))\n",
    "print(pred_2gram('the'))\n",
    "print(pred_2gram('cheshire'))\n",
    "print(pred_2gram('mock'))\n",
    "print(pred_2gram('cat'))\n",
    "print(pred_2gram('turtle'))\n",
    "    \n",
    "\n",
    "def classification_accuracy_2gram():\n",
    "    correctPred = 0\n",
    "    # D-1 because we don't have a next word for the last word\n",
    "    for i in range(0, D-1):\n",
    "        currWord = corpus[i]\n",
    "        pred = pred_2gram(currWord)[0]\n",
    "        nextWord = corpus[i+1]\n",
    "        if pred == nextWord:\n",
    "            correctPred += 1\n",
    "    return correctPred / (D-1)\n",
    "\n",
    "print('Classification Accuracy of the Bigram Classifier: ', classification_accuracy_2gram())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('well', 5.263736213123415e-11)\n",
      "('girl', 1.8513033175355449e-06)\n",
      "('miles', 1.3164823591363875e-05)\n",
      "Classification Accuracy of the 3-gram Classifier:  0.7551052652367974\n",
      "Classification Accuracy of the 3-gram Classifier:  0.7551052652367974\n",
      "Classification Accuracy of the 5-gram Classifier:  0.9424056883270788\n",
      "Classification Accuracy of the 5-gram Classifier:  0.9424056883270788\n",
      "Classification Accuracy of the 10-gram Classifier:  0.9962465428684314\n",
      "Classification Accuracy of the 10-gram Classifier:  0.9962465428684314\n"
     ]
    }
   ],
   "source": [
    "def CountsKGram(k):\n",
    "    counts_kgram = np.zeros((V,V))\n",
    "    for i in range(k,len(corpus)):\n",
    "        token_i = tokenize[corpus[i]]\n",
    "        token_imk = tokenize[corpus[i-k]]\n",
    "        counts_kgram[token_i,token_imk] += 1\n",
    "    return counts_kgram\n",
    "\n",
    "def get_likelihood_kgram(words):\n",
    "    if len(words) == 0:\n",
    "        raise ValueError('words must be a list of at least 1 word')\n",
    "    prior = get_likelihood_2gram(words[-1])\n",
    "\n",
    "    for i in range(1, len(words)):\n",
    "        k_gram = CountsKGram(i+1)\n",
    "        k_gram_t = k_gram.T\n",
    "        wordLoc = -i - 1\n",
    "        wordIndex = tokenize[words[wordLoc]]\n",
    "        row = k_gram_t[wordIndex]\n",
    "        wordCount = np.sum(k_gram_t, axis=1)\n",
    "        post = np.divide(row, wordCount, out=np.zeros_like(row), where=wordCount!=0)\n",
    "        prior *= post\n",
    "    return prior\n",
    "\n",
    "\n",
    "def pred_kgram(words):\n",
    "    likelihood = get_likelihood_kgram(words)\n",
    "    i = np.argmax(likelihood)\n",
    "    return(wordlist[i], likelihood[i])\n",
    "\n",
    "print(pred_kgram([ 'falling', 'down', 'a', 'very', 'deep']))\n",
    "print(pred_kgram(['what', 'an', 'ignorant', 'little']))\n",
    "print(pred_kgram(['four', 'thousand',]))\n",
    "\n",
    "\n",
    "def classification_accuracy_kgram(k):\n",
    "    correctPred = 0\n",
    "    for i in range(k, D):\n",
    "        currWords = corpus[i-k:i]\n",
    "        pred = pred_kgram(currWords)[0]\n",
    "        nextWord = corpus[i]\n",
    "        if pred == nextWord:\n",
    "            correctPred += 1\n",
    "    return correctPred / (D-k)\n",
    "\n",
    "# print('Classification Accuracy of the 1-gram Classifier: ', classification_accuracy_kgram(1))\n",
    "\n",
    "import threading\n",
    "\n",
    "def thread_function(k):\n",
    "    print('Classification Accuracy of the ' + str(k) + '-gram Classifier: ', classification_accuracy_kgram(k))\n",
    "\n",
    "x = threading.Thread(target=thread_function, args=(3,))\n",
    "y = threading.Thread(target=thread_function, args=(5,))\n",
    "z = threading.Thread(target=thread_function, args=(10,))\n",
    "x.start()\n",
    "y.start()\n",
    "z.start()\n",
    "x.join()\n",
    "y.join()\n",
    "z.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'mad', 'hatter', 'with', 'this', 'as', 'she', 'could', 'guess', 'she', 'was', 'now', 'about', 'two', 'feet', 'high', 'even', 'then', 'they', 'walked', 'off', 'together', 'alice', 'heard', 'a', 'little', 'pattering', 'of']\n"
     ]
    }
   ],
   "source": [
    "def text_generation_c():\n",
    "    # generate 25 words after the phrase 'the mad hatter'\n",
    "    phrase = ['the', 'mad', 'hatter']\n",
    "    for _ in range(25):\n",
    "        pred = pred_kgram(phrase[-3:])[0]\n",
    "        phrase.append(pred)\n",
    "    return phrase\n",
    "\n",
    "print(text_generation_c())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'mad', 'hatter', 'with', 'this', 'as', 'ever', 'was', 'in', 'the', 'pool', 'of', 'tears', 'which', 'she', 'had', 'wept', 'when', 'she', 'was', 'up', 'like', 'to', 'this', 'the', 'whole', 'pack', 'rose']\n"
     ]
    }
   ],
   "source": [
    "def text_generation_d():\n",
    "    # generate 25 words after the phrase 'the mad hatter' by sampling according to probability\n",
    "    phrase = ['the', 'mad', 'hatter']\n",
    "    for _ in range(25):\n",
    "        likelihood = get_likelihood_kgram(phrase[-3:])\n",
    "        pred = random.choices(wordlist, weights=likelihood, k=1)[0]\n",
    "        phrase.append(pred)\n",
    "    return phrase\n",
    "    \n",
    "print(text_generation_d())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0dc2fd043a8482d8110fc568899369c1e4bc80d83650aa995e9527605d230520"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
